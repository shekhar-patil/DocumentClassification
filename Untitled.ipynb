{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baa6546b-4afe-471f-af0b-bfd614d095cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shekhar-cardup/.local/lib/python3.10/site-packages/transformers/models/layoutlmv2/feature_extraction_layoutlmv2.py:30: FutureWarning: The class LayoutLMv2FeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use LayoutLMv2ImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([1, 103])\n",
      "bbox torch.Size([1, 103, 4])\n",
      "token_type_ids torch.Size([1, 103])\n",
      "attention_mask torch.Size([1, 103])\n",
      "image torch.Size([1, 3, 224, 224])\n",
      "[CLS] passport oh ) republic of singapore ph sor kara wong f 8 may 1977 30 ocr 2017 30 ocr 2022 see page 2 ratoni ne s77e8s8eh pasgpwong < < karasyunsens < < < < < < < < < < k < < < k < kkeke, k0000000Â£4sgp7705038f2210300s7788888h < < < < < 98 [SEP]\n",
      "Dataset({\n",
      "    features: ['image_path', 'label'],\n",
      "    num_rows: 7\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank_statement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/layoutlmv2-base-uncased were not used when initializing LayoutLMv2ForSequenceClassification: ['layoutlmv2.visual.backbone.bottom_up.res4.12.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.stem.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.shortcut.norm.num_batches_tracked']\n",
      "- This IS expected if you are initializing LayoutLMv2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LayoutLMv2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LayoutLMv2ForSequenceClassification were not initialized from the model checkpoint at microsoft/layoutlmv2-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b09abceaef412ea92d0a76ae0533b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/shekhar-cardup/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_11043/1686871865.py\", line 126, in <module>\n",
      "    outputs = model(**batch)\n",
      "  File \"/home/shekhar-cardup/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/shekhar-cardup/.local/lib/python3.10/site-packages/transformers/models/layoutlmv2/modeling_layoutlmv2.py\", line 1046, in forward\n",
      "    input_shape = input_ids.size()\n",
      "AttributeError: 'list' object has no attribute 'size'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shekhar-cardup/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/shekhar-cardup/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/shekhar-cardup/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/shekhar-cardup/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/shekhar-cardup/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1055, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/home/shekhar-cardup/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 955, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/home/shekhar-cardup/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 778, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/home/shekhar-cardup/.local/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/shekhar-cardup/.local/lib/python3.10/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/shekhar-cardup/.local/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/shekhar-cardup/.local/lib/python3.10/site-packages/stack_data/core.py\", line 677, in included_pieces\n",
      "    scope_pieces = self.scope_pieces\n",
      "  File \"/home/shekhar-cardup/.local/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/shekhar-cardup/.local/lib/python3.10/site-packages/stack_data/core.py\", line 614, in scope_pieces\n",
      "    scope_start, scope_end = self.source.line_range(self.scope)\n",
      "  File \"/home/shekhar-cardup/.local/lib/python3.10/site-packages/stack_data/core.py\", line 178, in line_range\n",
      "    return line_range(self.asttext(), node)\n",
      "  File \"/home/shekhar-cardup/.local/lib/python3.10/site-packages/executing/executing.py\", line 428, in asttext\n",
      "    self._asttext = ASTText(self.text, tree=self.tree, filename=self.filename)\n",
      "  File \"/home/shekhar-cardup/.local/lib/python3.10/site-packages/asttokens/asttokens.py\", line 307, in __init__\n",
      "    super(ASTText, self).__init__(source_text, filename)\n",
      "  File \"/home/shekhar-cardup/.local/lib/python3.10/site-packages/asttokens/asttokens.py\", line 44, in __init__\n",
      "    source_text = six.ensure_text(source_text)\n",
      "AttributeError: module 'six' has no attribute 'ensure_text'\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "from transformers import LayoutLMv2FeatureExtractor, LayoutLMv2Tokenizer, LayoutLMv2Processor\n",
    "import pandas as pd\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from datasets import Features, Sequence, ClassLabel, Value, Array2D, Array3D\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "image = Image.open(\"./content/RVL_CDIP_one_example_per_class/passport/passport1.jpg\")\n",
    "image = image.convert(\"RGB\")\n",
    "\n",
    "\n",
    "# converted text\n",
    "\n",
    "# this is manual code\n",
    "# ocr_df = pytesseract.image_to_data(image, output_type='data.frame')\n",
    "# ocr_df = ocr_df.dropna().reset_index(drop=True)\n",
    "# float_cols = ocr_df.select_dtypes('float').columns\n",
    "# ocr_df[float_cols] = ocr_df[float_cols].round(0).astype(int)\n",
    "# ocr_df = ocr_df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "# words = ' '.join([word for word in ocr_df.text if str(word) != 'nan'])\n",
    "\n",
    "\n",
    "# print(words)\n",
    "\n",
    "\n",
    "feature_extractor = LayoutLMv2FeatureExtractor()\n",
    "tokenizer = LayoutLMv2Tokenizer.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n",
    "processor = LayoutLMv2Processor(feature_extractor, tokenizer)\n",
    "\n",
    "encoded_inputs = processor(image, return_tensors=\"pt\")\n",
    "for k,v in encoded_inputs.items():\n",
    "  print(k, v.shape)\n",
    "# converted text\n",
    "print(processor.tokenizer.decode(encoded_inputs.input_ids.squeeze().tolist()))\n",
    "\n",
    "dataset_path = \"./content/RVL_CDIP_one_example_per_class\"\n",
    "labels = [label for label in os.listdir(dataset_path)]\n",
    "id2label = {v: k for v, k in enumerate(labels)}\n",
    "label2id = {k: v for v, k in enumerate(labels)}\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for label_folder, _, file_names in os.walk(dataset_path):\n",
    "  if label_folder != dataset_path:\n",
    "    label = label_folder[40:]\n",
    "    label = label[1:]\n",
    "    for _, _, image_names in os.walk(label_folder):\n",
    "      relative_image_names = []\n",
    "      for image_file in image_names:\n",
    "        relative_image_names.append(dataset_path + \"/\" + label + \"/\" + image_file)\n",
    "      images.extend(relative_image_names)\n",
    "      labels.extend([label] * len (relative_image_names))\n",
    "\n",
    "data = pd.DataFrame.from_dict({'image_path': images, 'label': labels})\n",
    "data.head()\n",
    "\n",
    "\n",
    "\n",
    "# read dataframe as HuggingFace Datasets object\n",
    "dataset = Dataset.from_pandas(data)\n",
    "print(dataset)\n",
    "\n",
    "\n",
    "# we need to define custom features\n",
    "features = Features({\n",
    "    'image': Array3D(dtype=\"int64\", shape=(3, 224, 224)),\n",
    "    'input_ids': Sequence(feature=Value(dtype='int64')),\n",
    "    'attention_mask': Sequence(Value(dtype='int64')),\n",
    "    'token_type_ids': Sequence(Value(dtype='int64')),\n",
    "    'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),\n",
    "    'labels': ClassLabel(num_classes=len(np.unique(labels).tolist()), names=np.unique(labels).tolist()),\n",
    "})\n",
    "\n",
    "def preprocess_data(examples):\n",
    "  # take a batch of images\n",
    "  images = [Image.open(path).convert(\"RGB\") for path in examples['image_path']]\n",
    "\n",
    "  encoded_inputs = processor(images, padding=\"max_length\", truncation=True)\n",
    "\n",
    "  # add labels\n",
    "  encoded_inputs[\"labels\"] = [label2id[label] for label in examples[\"label\"]]\n",
    "\n",
    "  return encoded_inputs\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_data, remove_columns=dataset.column_names, features=features, batched=True, batch_size=2)\n",
    "# encoded_dataset.set_format(type=\"torch\")\n",
    "\n",
    "import torch\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(encoded_dataset, batch_size=4)\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "# for k,v in batch.items():\n",
    "  # print(k, len(v), v[0][0].shape)\n",
    "\n",
    "processor.tokenizer.decode(batch['input_ids'][0].tolist())\n",
    "print(id2label[batch['labels'][0].item()])\n",
    "\n",
    "from transformers import LayoutLMv2ForSequenceClassification\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LayoutLMv2ForSequenceClassification.from_pretrained(\"microsoft/layoutlmv2-base-uncased\", num_labels=len(labels))\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "global_step = 0\n",
    "num_train_epochs = 10\n",
    "t_total = len(dataloader) * num_train_epochs # total number of training steps\n",
    "\n",
    "#put the model in training mode\n",
    "model.train()\n",
    "for epoch in range(num_train_epochs):\n",
    "  print(\"Epoch:\", epoch)\n",
    "  running_loss = 0.0\n",
    "  correct = 0\n",
    "  for batch in tqdm(dataloader):\n",
    "    # forward pass\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "\n",
    "    running_loss += loss.item()\n",
    "    predictions = outputs.logits.argmax(-1)\n",
    "    correct += (predictions == batch['labels']).float().sum()\n",
    "\n",
    "    # backward pass to get the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    global_step += 1\n",
    "\n",
    "  print(\"Loss:\", running_loss / batch[\"input_ids\"].shape[0])\n",
    "  accuracy = 100 * correct / len(data)\n",
    "  print(\"Training accuracy:\", accuracy.item())\n",
    "\n",
    "\n",
    "# prepare image for the model\n",
    "encoded_inputs = processor(image, return_tensors=\"pt\")\n",
    "\n",
    "# make sure all keys of encoded_inputs are on the same device as the model\n",
    "for k,v in encoded_inputs.items():\n",
    "  encoded_inputs[k] = v.to(model.device)\n",
    "\n",
    "# forward pass\n",
    "outputs = model(**encoded_inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "print(logits.shape)\n",
    "\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", id2label[predicted_class_idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692b726d-691e-4570-b999-349ce69447e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
